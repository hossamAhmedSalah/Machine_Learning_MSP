{"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/hossamahmedsalah/ensemble-learning-msp?scriptVersionId=142791575\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"<div style=\"padding: 35px;color:white;margin:10;font-size:200%;text-align:center;display:fill;border-radius:10px;overflow:hidden;background-image: url(https://github.com/hossamAhmedSalah/Machine_Learning_MSP/blob/main/Assets/247.jpg?raw=true?)\">\n<b>\n<span style='color:skyblue'>MSP Machine Learning workshop 2023 </span>\n</b>\n<div>\n<span style='color:Salmon'>Ensemble Learning</span>\n\n</div>\n\n</div>\n\n<br>\n","metadata":{}},{"cell_type":"markdown","source":"# You can view other sessions via \n[GitHub - hossamAhmedSalah/Machine_Learning_MSP: MSP 23 workshop of machine learning](https://github.com/hossamAhmedSalah/Machine_Learning_MSP/tree/main)\n![MSP Logo](https://github.com/hossamAhmedSalah/Machine_Learning_MSP/blob/main/Assets/image-removebg-preview.png?raw=true)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"key\"></a>\n# Table of Content \n1) [importing libraries](#1)\n\n2) [Reading the dataset](#2)\n   - 2.1 [Heart datasetðŸ’–](#21)\n   \n3) [Preprocessing](#3)\n   - 3.1 [Encoding](#3.1)\n   - 3.2 [Why we don't need to scale the data in treeðŸŒ³_based models](#3.2)\n   \n4) [EDA](#eda)\n\n5) [Preparing the data](#4)\n   - 4.1 [Spliting the data into X features and y the target](#4.1)\n   - 4.2 [Spliting the data into Train and Test](#4.2)\n   \n6) [Modeling](#6)\n   - 6.1 [Max Voting](#6.1)\n   \n7) [Random Forest](#7)\n   - [Bootstrapping](#7.1)\n   - [How the Forest work?](#7.2) \n   - [Grid search](#7.3)\n   - [Out of Bag (OOB) score](#7.4)\n   \n8) [Ada Boost](#8)\n   - [Ada Boost Parameters](#8.1)\n   - [Grid Search](#8.2)","metadata":{}},{"cell_type":"markdown","source":"# <a id=\"1\">importing libraries </a>\n[ðŸŒŸgo to table of contentðŸŒŸ](#key)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n# from the ensemble model we import some classifiers\nfrom sklearn.ensemble import (\n    RandomForestClassifier, \n    ExtraTreesClassifier,\n    VotingClassifier,\n    AdaBoostClassifier, \n    GradientBoostingClassifier,\n    )\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id=\"2\">Reading the dataset </a>\n[ðŸŒŸgo to table of contentðŸŒŸ](#key)","metadata":{}},{"cell_type":"code","source":"# reading the dataset \nheart = pd.read_csv('/kaggle/input/heart-failure-prediction/heart.csv')\nheart","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id=\"21\">Heart datasetðŸ’–</a>\n1. Age: age of the patient [years]\n2. Sex: sex of the patient [M: Male, F: Female]\n3. ChestPainType: chest pain type [TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic]\n4. RestingBP: resting blood pressure [mm Hg]\n5. Cholesterol: serum cholesterol [mm/dl]\n6. FastingBS: fasting blood sugar [1: if FastingBS > 120 mg/dl, 0: otherwise]\n7. RestingECG: resting electrocardiogram results [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria]\n8. MaxHR: maximum heart rate achieved [Numeric value between 60 and 202]\n9. ExerciseAngina: exercise-induced angina [Y: Yes, N: No]\n10. Oldpeak: oldpeak = ST [Numeric value measured in depression]\n11. ST_Slope: the slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]\n12. <span style= \"color :red\">HeartDisease: output class [1: heart disease, 0: Normal]</span>\n\n[ðŸŒŸgo to table of contentðŸŒŸ](#key)","metadata":{}},{"cell_type":"code","source":"# dataset info\n# no messing values\nheart.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# describing \nheart.describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# describing the categories \nheart.describe(include='O')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <a id=\"3\">Preprocessing</a>\n[ðŸŒŸgo to table of contentðŸŒŸ](#key)","metadata":{}},{"cell_type":"markdown","source":"## <a id=\"31\">Encoding</a>\n","metadata":{}},{"cell_type":"code","source":"# selecting columns that would be encoded\nheart.select_dtypes(include=\"O\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# selecting columns that would be encoded\ncols = heart.select_dtypes(include=\"O\").columns\ncols","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Encoding \nfrom sklearn.preprocessing import LabelEncoder\n\n# initiating the encoder class\nlenc = LabelEncoder()\n\n# looping over each columns from the object type columns\nfor col in cols:\n    # encode these columns\n    heart[col] = lenc.fit_transform(heart[col])\n\n# diplaying \nheart.head(4)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id=\"32\">Why we don't need to scale the data in treeðŸŒ³_based models</a>?\n","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<div style=\"border-radius:10px;border:salmon solid;padding: 15px;background-color:#ffffff00;font-size:100%;text-align:left\">\n    Scaling is a common preprocessing step for many machine learning algorithms, particularly those that rely on <strong><mark style =\"background-color:salmon;color:white;border-radius:4px;opacity:1.0\">distance-based metrics</mark></strong> or <mark style=\"background-color:salmon;border-radius:4px;opacity:1.0;color:white\"><strong>gradient-based optimization methods</strong></mark>. However, tree-based models work differently, and their inherent structure and algorithmic characteristics make scaling unnecessary\n</div>\n\n\n<br>","metadata":{}},{"cell_type":"markdown","source":"# <a id=\"eda\">EDA</a>","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nsns.pairplot(data=heart, hue=\"HeartDisease\");","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(10,5), dpi=170)\nsns.heatmap(heart.corr(), cmap=\"Blues\", annot=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id=\"4\">Preparing the data</a>","metadata":{}},{"cell_type":"markdown","source":"# <a id=\"41\">Spliting the data into X features and y the target</a>","metadata":{}},{"cell_type":"code","source":"X = heart.drop(columns=[\"HeartDisease\"])\ny = heart[\"HeartDisease\"]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id=\"42\">Spliting the data into Train and Test</a>","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# spliting the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id=\"6\">Modeling</a>","metadata":{}},{"cell_type":"markdown","source":"## <a id=\"61\">Max Voting</a>","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1 = LogisticRegression(max_iter=1000)\n# you may find a warning when using Logistic regression here that the solver didn't converge and reached Limit \n# so I make max_iter=1000 that may be a side effect of not scalling the data for logistic regression\nmodel1.fit(X_train, y_train)\nprint(f\"Training score {model1.score(X_train, y_train)}\")\nprint(f\"Cross Validation {cross_val_score(model1, X_train, y_train, cv=5, n_jobs=-1).mean()}\")\nprint(f\"Testing score {model1.score(X_test, y_test)}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model2 = DecisionTreeClassifier()\nmodel2.fit(X_train, y_train)\nprint(f\"Training score {model2.score(X_train, y_train)}\")\nprint(f\"Cross Validation {cross_val_score(model2, X_train, y_train, cv=5, n_jobs=-1).mean()}\")\nprint(f\"Testing score {model2.score(X_test, y_test)}\")\n# overfitting \n# here you may notice how Cross validation is useful","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's compine these models\ncombined = VotingClassifier(estimators=[('lr',model1), ('dt', model2)], voting='hard')\ncombined.fit(X_train, y_train)\n\nprint(f\"Training score {combined.score(X_train, y_train)}\")\nprint(f\"Cross Validation {cross_val_score(combined, X_train, y_train, cv=5, n_jobs=-1).mean()}\")\nprint(f\"Testing score {combined.score(X_test, y_test)}\")\n# we could reduce the overfitting 7% and we can do more better if we used more models \n# Cross Validation also show the real efficiency of the model over the training set that was almost like the real test data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id=\"7\"> Random Forest ðŸŒ²ðŸŒ²ðŸŒ²</a>","metadata":{}},{"cell_type":"markdown","source":"## <a id=\"71\">Bootstrapping</a>","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<div style=\"border-radius:10px;border:lightgreen solid;padding: 15px;background-color:#ffffff00;font-size:100%;text-align:left\">\n   <strong>Random Sampling <mark style=\"color:white; background-color:lightgreen; border-radius:4px;opacity:1.0\">with replacement</mark><strong>, allowing the observations to be chosen multiple times \n</div>\n\n\n<br>","metadata":{}},{"cell_type":"markdown","source":"![Bootstrapping](https://github.com/hossamAhmedSalah/Machine_Learning_MSP/blob/main/Assets/OutOfBag.jpg?raw=true)","metadata":{}},{"cell_type":"markdown","source":"## <a id=\"72\">How the Forest work?</a>","metadata":{}},{"cell_type":"markdown","source":"![Forest](https://github.com/hossamAhmedSalah/Machine_Learning_MSP/blob/main/Assets/RandomForestBagging.jpg?raw=true)","metadata":{}},{"cell_type":"code","source":"model3 = RandomForestClassifier()\nmodel3.fit(X_train, y_train)\n\n# evaluating\nprint(f\"Training score {model3.score(X_train, y_train)}\")\nprint(f\"Cross Validation {cross_val_score(model3, X_train, y_train, cv=5, n_jobs=-1).mean()}\")\nprint(f\"Testing score {model3.score(X_test, y_test)}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mo = RandomForestClassifier(ccp_alpha=0.01)\nmo.fit(X_train, y_train)\n\n# evaluating\nprint(f\"Training score {mo.score(X_train, y_train)}\")\nprint(f\"Cross Validation {cross_val_score(mo, X_train, y_train, cv=5, n_jobs=-1).mean()}\")\nprint(f\"Testing score {mo.score(X_test, y_test)}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mo1 = RandomForestClassifier(ccp_alpha=0.001)\nmo1.fit(X_train, y_train)\n\n# evaluating\nprint(f\"Training score {mo1.score(X_train, y_train)}\")\nprint(f\"Cross Validation {cross_val_score(mo1, X_train, y_train, cv=5, n_jobs=-1).mean()}\")\nprint(f\"Testing score {mo1.score(X_test, y_test)}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mo2 = RandomForestClassifier(n_estimators=70)\nmo2.fit(X_train, y_train)\n\n# evaluating\nprint(f\"Training score {mo2.score(X_train, y_train)}\")\nprint(f\"Cross Validation {cross_val_score(mo2, X_train, y_train, cv=5, n_jobs=-1).mean()}\")\nprint(f\"Testing score {mo2.score(X_test, y_test)}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# how about max voting ðŸ«£ \nvoter = VotingClassifier(estimators=[('mo', mo), ('mo1', mo1), ('mo2', mo2)], n_jobs=-1, voting='hard')\n\nvoter.fit(X_train, y_train)\n# evaluating\nprint(f\"Training score {voter.score(X_train, y_train)}\")\nprint(f\"Cross Validation {cross_val_score(voter, X_train, y_train, cv=5, n_jobs=-1).mean()}\")\nprint(f\"Testing score {voter.score(X_test, y_test)}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id=\"73\">Grid Search</a>","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nrf = RandomForestClassifier(random_state=42)\n# Specify the hyperparameters to search over\nparam_grid = {\n    'n_estimators': [100, 200, 300],  # Number of trees in the forest\n    'max_features': ['auto', 'sqrt', 'log2'],  # Number of features to consider at each split\n    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split a node\n    'min_samples_leaf': [1, 2, 4]  # Minimum number of samples required at each leaf node\n}\n\n# Create a grid search instance\ngrid_search = GridSearchCV(estimator=rf, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n\n# Fit the grid search to your training data\ngrid_search.fit(X_train, y_train)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n<div style=\"border-radius:10px;border:skyblue solid;padding: 15px;background-color:#ffffff00;font-size:100%;text-align:left\">\nThe max_features hyperparameter in scikit-learn's Random Forest and Decision Tree models controls the number of features to consider when making a split at each node of a tree. It can take on several values, including:\n\n1. **`'auto'`**: This option lets the algorithm automatically determine the maximum number of features to consider for each split. It sets **`max_features`** to the <strong><mark style=\"background-color:skyblue;color:white;border-radius:4px;opacity:1.0;\">square root of the total number of features</mark></strong>. So, if you have 16 features, **`'auto'`** would set **`max_features`** to 4. This is the default setting.\n2. **`'sqrt'`**: Similar to **`'auto'`**, **`'sqrt'`** also sets **`max_features`** to the square root of the total number of features.\n3. **`'log2'`**: This option sets **`max_features`** to the base-2 logarithm of the total number of features. It's another way to control the number of features considered at each split.\n4. An integer value: You can specify an integer value for **`max_features`**, indicating the strong><strong><mark style=\"background-color:skyblue;color:white;border-radius:4px;opacity:1.0;\">exact number of features to consider at each split</mark></strong>. For example, **`max_features=10`** would consider 10 features at each split.\n5. A float value between 0 and 1: If you specify a floating-point value between 0 and 1 (e.g., **`max_features=0.5`**), the algorithm will consider that <strong><mark style=\"background-color:skyblue;color:white;border-radius:4px;opacity:1.0;\">fraction of the total features at each split</mark></strong>. This can be useful for controlling feature selection.\n\nThe choice of **`max_features`** can impact the behavior of the Random Forest or Decision Tree model. Using smaller values can reduce overfitting, while using larger values can lead to more complex trees and potentially overfitting on the training data. The **`'auto'`**, **`'sqrt'`**, and **`'log2'`** settings are convenient because they adapt to the number of features in your dataset.\n</div>\n</br>","metadata":{}},{"cell_type":"code","source":"# Get the best model and its hyperparameters\nbest_rf = grid_search.best_estimator_\nbest_params = grid_search.best_params_\nbest_score = grid_search.best_score_\n\n# Evaluate the best model on your test data\ntest_accuracy = best_rf.score(X_test, y_test)\n\n# Print the best hyperparameters and test accuracy\nprint(\"Best Hyperparameters:\", best_params)\nprint(\"Best Cross-Validation Accuracy:\", best_score)\nprint(\"Test Accuracy:\", test_accuracy)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(grid_search.cv_results_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_search.best_params_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <a id=\"74\">Out Of Bag score (OOB)</a>","metadata":{}},{"cell_type":"code","source":"model4 = RandomForestClassifier(random_state=42, min_samples_leaf=2, min_samples_split=10, n_estimators=100, oob_score=True)\n# Fit the model to the data\nmodel4.fit(X_train, y_train)\n\n# Access the OOB score (accuracy)\noob_score = model4.oob_score_\nprint(\"OOB Score:\", oob_score)\n# note if you didn't state oob_score=True\n# and tried to do oob score an error would rise as the model has no attribute called oob_score (false default)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id= \"8\">Ada Boost</a>","metadata":{}},{"cell_type":"code","source":"boost_model = AdaBoostClassifier(random_state=42)\n\n# fitting the model\nboost_model.fit(X_train, y_train)\n\n# Evaluating \nprint(f\"Training score {boost_model.score(X_train, y_train)}\")\nprint(f\"Cross Validation {cross_val_score(boost_model, X_train, y_train, cv=5, n_jobs=-1).mean()}\")\nprint(f\"Testing score {boost_model.score(X_test, y_test)}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a id=\"81\">Ada Boost Parameters</a> ","metadata":{}},{"cell_type":"markdown","source":"1. **`n_estimators`**:\n    - This hyperparameter determines the number of weak learners (base estimators) to train. Increasing **`n_estimators`** can lead to a more powerful ensemble, but it may also increase the risk of overfitting. You can perform cross-validation or use other methods to find an appropriate value for this parameter.\n2. **`base_estimator`**:\n    - AdaBoost can use different base estimators (e.g., DecisionTreeClassifier, RandomForestClassifier, etc.). The choice of the base estimator should depend on the characteristics of your data. For example, if your data is non-linear, using a decision tree with deep branching might be suitable.\n3. **`learning_rate`**:\n    - The learning rate shrinks the contribution of each weak learner in the ensemble. A smaller learning rate (e.g., 0.1 or lower) may improve generalization but require more estimators. A larger learning rate (e.g., 1.0) makes the ensemble learn faster but can lead to overfitting.\n4. **`algorithm`**:\n    - AdaBoost has two algorithms: 'SAMME' (default) and 'SAMME.R'. 'SAMME.R' can handle continuous and multiclass targets and typically performs better. However, it requires that the base estimator has **`predict_proba`** method to compute class probabilities.\n5. **`random_state`**:\n    - Set a random seed (**`random_state`**) for reproducibility if needed.\n6. **Base Estimator Parameters**:\n    - If you use a decision tree as the base estimator, you can also tune its hyperparameters, such as **`max_depth`**, **`min_samples_split`**, and **`min_samples_leaf`**, to control the complexity of the individual trees.\n7. **Feature Scaling and Preprocessing**:\n    - Depending on the choice of base estimator, you might need to consider feature scaling and preprocessing steps that are suitable for that estimator.","metadata":{}},{"cell_type":"markdown","source":"## <a id=\"82\"> Grid search </a>","metadata":{}},{"cell_type":"code","source":"# Create an AdaBoostClassifier instance\nada_boost = AdaBoostClassifier(random_state=42)\n\n# Define the hyperparameters and their possible values for the grid search\nparam_grid = {\n    'base_estimator': [DecisionTreeClassifier(max_depth=1), LogisticRegression()],\n    'n_estimators': [50, 100, 200],\n    'learning_rate': [0.1, 0.5, 1.0],\n}\n\n# Create a GridSearchCV instance\ngrid_search = GridSearchCV(estimator=ada_boost, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n\n# Fit the grid search to your training data\ngrid_search.fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model = grid_search.best_estimator_\nbest_model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model.score(X_test, y_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model.score(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"importance = best_model.feature_importances_\nfeatures = X_train.columns\nplt.figure(figsize=(10,10))\nplt.bar(features, importance)\nplt.xticks(rotation = 45);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = [None for i in range(2)]\n\nplt.figure(figsize=(15,7), dpi=200)\n\n# making the structure \nax[0] = plt.subplot2grid((1, 2), (0, 0), colspan=1)\nax[1] = plt.subplot2grid((1, 2), (0, 1), colspan=1)\n\n# seaborn correlation heatmap at ax[0]\nsns.heatmap(data=heart.corr(), annot=True, cmap=\"Blues\", ax=ax[0], fmt=\".2f\")\n\n# feature importance at ax[1]\nax[1].bar(features, importance)\nax[1].set_xticks(range(len(features)))\nax[1].set_xticklabels(features, rotation=45)  # Use set_xticklabels to rotate x-axis labels\nax[1].set_title(\"Feature importance\")  # Use set_title to set the title\n\n# Adjust layout and display the plots\nplt.tight_layout()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[ðŸŒŸgo to table of contentðŸŒŸ](#key)","metadata":{}}],"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}}