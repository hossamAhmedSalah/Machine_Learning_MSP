{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/hossamahmedsalah/ica-pca-k-means-msp?scriptVersionId=143240799\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"<div style=\"padding: 35px;color:white;margin:10;font-size:200%;text-align:center;display:fill;border-radius:10px;overflow:hidden;background-image: url(https://github.com/hossamAhmedSalah/Machine_Learning_MSP/blob/main/Assets/247.jpg?raw=true?)\">\n<b>\n<span style='color:skyblue'>MSP Machine Learning workshop 2023 </span>\n</b>\n<div>\n<span style='color:Salmon'>UnsuperVised Learning</span>\n\n</div>\n\n</div>\n\n<br>\n","metadata":{}},{"cell_type":"markdown","source":"# You can view other sessions via \n[GitHub - hossamAhmedSalah/Machine_Learning_MSP: MSP 23 workshop of machine learning](https://github.com/hossamAhmedSalah/Machine_Learning_MSP/tree/main)\n![MSP Logo](https://github.com/hossamAhmedSalah/Machine_Learning_MSP/blob/main/Assets/image-removebg-preview.png?raw=true)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"key\"></a>\n# Table of Content \n1) [ICA](#1)\n2) [Mixed Music datasetüéµ](#2)\n   - 2.1 [How to deal with Audioüîâ](#21)\n3) [Visualizing Audio Filesüéµ](#3)\n   - 3.1 [Listen to the file üëÇ](#3.1)\n   - 3.2 [Reading 2 other music files](#3.2)\n4) [Summaryüé∂](#4)\n5) [Creating the dataset from the music files](#5)\n6) [Applying the ICA](#6)\n   - 6.1 [Results of the ICA](#6.1)\n7) [PCA](#7)\n8) [MNIST dataset 0Ô∏è‚É£9Ô∏è‚É£](#8)\n9) [KMeans](#9)","metadata":{}},{"cell_type":"markdown","source":"# <span style=\"color : skyblue\" id=\"1\">ICA</span>\n> Independent component analysis\n\n>Independent Component Analysis (ICA) is a computational technique used in signal processing and data analysis to separate a multivariate signal into additive, statistically independent components. It is a method for blind source separation, which means it can separate a set of mixed signals into their original source signals without knowing the characteristics of the sources or the mixing process.","metadata":{}},{"cell_type":"markdown","source":"ICA has applications in various fields, including:\n\n1. **Audio Source Separation**: Separating different sound sources (e.g., instruments or voices) from a mixed audio recording.\n2. **Image Processing**: Extracting independent features or sources from mixed images.\n3. **Neuroscience**: Analyzing brain signals (e.g., EEG or fMRI data) to identify independent neural sources.\n4. **Financial Data Analysis**: Separating underlying factors or market trends from financial time series data.\n5. **Speech Recognition**: Separating different speakers' voices in a recorded conversation.","metadata":{}},{"cell_type":"markdown","source":"# <span style=\"color:skyblue\" id=\"2\">Mixed Music datasetüéµ</span>\nWe have three WAVE files, each of which is a mix","metadata":{}},{"cell_type":"markdown","source":"## <h2 id=\"21\">How to deal with Audioüîâ</h2>\nDealing with audio files in Python involves various tasks such as reading, writing, processing, and analyzing audio data. To work with audio files in Python, you can use libraries like `librosa`, `soundfile`, and `pydub`. \n\nYou can process audio data using libraries like `numpy` . For example, you can apply filters, perform Fourier transforms, or manipulate audio signals.\n\n```python\nimport numpy as np\n\n# Example: Apply a simple gain (volume) change\naudio_data *= 2.0  # Increase volume by a factor of 2\n\n```\n    \n- **Audio Playback**:\n    \n    To play audio directly from Python, you can use the `pydub` library. Install it first:\n    \n    ```bash\n    pip install pydub\n    \n    ```\n    \n    Then, you can play audio like this:\n    \n    ```python\n    from pydub import AudioSegment\n    from pydub.playback import play\n    \n    audio = AudioSegment.from_file(\"your_audio.wav\")\n    play(audio)\n    \n    ```\n    ","metadata":{}},{"cell_type":"code","source":"from pydub import AudioSegment\nimport IPython\nimport numpy as np\nimport wave","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"audio1 = wave.open('/kaggle/input/icamusical/ICA mix 1.wav', 'r')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"audio1.getparams()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. **Mono (1 Channel)**:\n    - Mono audio contains a single audio channel.\n    - It is often used for voice recordings, podcasts, and some types of music.\n2. **Stereo (2 Channels)**:\n    - Stereo audio consists of two audio channels: a left channel and a right channel.\n    - Stereo is used for most music recordings, as it allows for the creation of a spatial or directional audio experience.\n3. **Multichannel (More than 2 Channels)**:\n    - Multichannel audio can have more than two channels, such as 5.1 (five primary channels plus a subwoofer channel) or 7.1 (seven primary channels plus a subwoofer channel).\n    - It is commonly used in home theater systems, surround sound setups, and advanced audio production.\n>https://youtu.be/48ExvlterwY?si=52cWXKOgUgMGbaGb üì∫ this video can be helpful","metadata":{}},{"cell_type":"markdown","source":"1. **`nchannels`**: This parameter specifies the number of audio channels in the audio file. In this case, it's set to **`1`**, indicating that the audio is monaural (single channel).\n2. **`sampwidth`**: **`sampwidth`** specifies the sample width or the number of bytes used to represent each audio sample. A value of **`2`** typically represents 16-bit audio, which is a common format for CD-quality audio.\n3. **`framerate`**: **`framerate`** indicates the number of samples (audio data points) recorded per second. In this case, it's set to **`44100`**, which is the standard sample rate for audio CDs and is often used for high-quality audio recordings.\n4. **`nframes`**: **`nframes`** represents the total number of audio frames or samples in the audio file. In your example, it's set to **`264515`**, which means the audio file contains 264,515 audio frames.\n5. **`comptype`**: This parameter specifies the compression type used for the audio file. In this case, it's **`'NONE'`**, indicating that the audio is not compressed. Uncompressed audio is common for high-quality audio recordings.\n6. **`compname`**: **`compname`** provides the name of the compression scheme used, and in this case, it's **`'not compressed'`**, which corresponds to the fact that the audio is not compressed","metadata":{}},{"cell_type":"markdown","source":">So this file has only channel which means it's mono sound.\n>It has a frame rate of 44100, which means each second of sound is represented by 44100 integers (integers because the file is in the common PCM 16-bit format).\n>The file has a total of 264515 integers/frames, which means its length in seconds is:`264515/44100 = 5.998072562358277`","metadata":{}},{"cell_type":"code","source":"# extract the frames of the wave file, which will be a part of the dataset we'll run ICA against\n'''\naudio1.readframes(-1) reads all the audio frames from the audio1 object. \nThe -1 argument means to read all frames until the end of the file.\n This line retrieves the audio data in its raw binary form.  \n'''\nsignal_1_raw = audio1.readframes(-1)\n'''\nnp.frombuffer(signal_1_raw, dtype=np.int16) creates a NumPy array signal_1 from the binary data signal_1_raw and specifies the data type as np.int16, \nwhich corresponds to 16-bit signed integers. \n'''\nsignal_1 = np.frombuffer(signal_1_raw, dtype=np.int16)\n# signal_1 will contain the audio data as a NumPy array of 16-bit integers, which can be easily manipulated, analyzed, or visualized ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(signal_1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"signal_1[:100]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <h1 style=\"color:skyblue\" id=\"3\">Visualizing Audio Filesüéµ</h1>","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n'''\nmix_1_wave.getframerate() retrieves the sample rate (in Hz) of the audio file represented by the mix_1_wave object and assigns it to the variable fs.\nThe sample rate represents the number of audio samples recorded per second.  \n'''\nfs = audio1.getframerate()\n'''\nlen(signal_1) calculates the length of the signal_1 NumPy array, which corresponds to the number of audio samples in the audio file.\nnp.linspace(0, len(signal_1)/fs, num=len(signal_1)) creates an array timing containing evenly spaced time values.\n These time values correspond to the time axis of the audio waveform plot. The range of time values is determined by the length of the audio file and the sample rate \n'''\ntiming = np.linspace(0, len(signal_1)/fs, num=len(signal_1))\n\nplt.figure(figsize=(12,2))\nplt.title('Audio 1')\nplt.plot(timing,signal_1, c=\"salmon\")\nplt.ylim(-35000, 35000)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- **`np.linspace(start, stop, num)`**: This NumPy function generates an array of evenly spaced values over a specified range.\n    - **`start`**: The starting value of the sequence.\n    - **`stop`**: The end value of the sequence.\n    - **`num`**: The number of evenly spaced values to generate.\n\nIn your code:\n\n- **`start`** is set to **`0`**, indicating that you want the time values to start from 0 seconds.\n- **`stop`** is calculated as **`len(signal_1)/fs`**, where:\n    - **`len(signal_1)`** is the length of the **`signal_1`** array, which corresponds to the number of audio samples.\n    - **`fs`** is the sample rate of the audio, which represents the number of samples per second.\n    - **`len(signal_1)/fs`** calculates the duration of the audio in seconds. It's the total number of audio samples divided by the sample rate, resulting in the duration of the audio recording in seconds.\n- **`num`** is set to **`len(signal_1)`**, which specifies that you want to generate as many time values as there are audio samples in the **`signal_1`** array. Each time value corresponds to a specific audio sample.","metadata":{}},{"cell_type":"markdown","source":"## <h2 style=\"color:skyblue\" id =\"31\">Listen to the file üëÇ</h2>","metadata":{}},{"cell_type":"code","source":"IPython.display.Audio('/kaggle/input/icamusical/ICA mix 1.wav') # üéπ piano","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <h2 style=\"color:skyblue\" id =\"32\">Reading 2 other music files</h2>","metadata":{}},{"cell_type":"code","source":"# audio2\naudio2 = wave.open('/kaggle/input/icamusical/ICA mix 2.wav', 'r')\n\n# Extract the frames\nsignal_2_raw = audio2.readframes(-1)\nsignal_2 = np.frombuffer(signal_2_raw, dtype=np.int16)\n\n# ploting üîâ \nplt.figure(figsize=(12,2))\nplt.plot(timing, signal_2, c=\"skyblue\")\nplt.title(\"Audio 2\")\nplt.ylim(-35000, 35000)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max(signal_2), min(signal_2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# listening üëÇ \nIPython.display.Audio('/kaggle/input/icamusical/ICA mix 2.wav') ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# audio 3\naudio3 = wave.open('/kaggle/input/icamusical/ICA mix 3.wav', 'r')\n\n# Extract the frames\nsignal_3_raw = audio3.readframes(-1)\nsignal_3 = np.frombuffer(signal_3_raw, dtype=np.int16)\n\n# ploting \nplt.figure(figsize=(12,2))\nplt.plot(timing, signal_3, c=\"lightgreen\")\nplt.title(\"Audio 3\")\nplt.ylim(-35000, 35000)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Listening üëÇ \nIPython.display.Audio('/kaggle/input/icamusical/ICA mix 3.wav')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <h2 style=\"color:skyblue\" id=\"4\">Summaryüé∂</h2>","metadata":{}},{"cell_type":"code","source":"ax = [None for _ in range(3)]\nplt.figure(figsize=(20, 10))\n\n\nax[0] = plt.subplot2grid((3,1), (0,0))\nax[0].plot(timing, signal_1, c=\"salmon\")\nax[0].set_title(\"Audio 1\")\n\nax[1] = plt.subplot2grid((3,1), (1,0))\nax[1].plot(timing, signal_2, c=\"skyblue\")\nax[1].set_title(\"Audio 2\")\n\nax[2] = plt.subplot2grid((3,1), (2,0))\nax[2].plot(timing, signal_3, c=\"lightgreen\")\nax[2].set_title(\"Audio 3\")\n\n# setting the ylim \nylim = (-35000, 35000)\nfor i in range(3):\n    ax[i].set_ylim(ylim)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import Audio, display\n\n# List of audio file paths\naudio_files = ['/kaggle/input/icamusical/ICA mix 1.wav', '/kaggle/input/icamusical/ICA mix 2.wav', '/kaggle/input/icamusical/ICA mix 3.wav']\n\n# Display audio files vertically\nfor audio_file in audio_files:\n    print(audio_file)\n    display(Audio(audio_file))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <h1 style=\"color:skyblue\" id=\"5\">Creating the dataset from the music files</h1>","metadata":{}},{"cell_type":"markdown","source":"ex1\n```python\na = np.array([1, 2, 3])\nb = np.array([4, 5, 6])\nnp.vstack((a,b))\n\narray([[1, 2, 3],\n       [4, 5, 6]])\n```\nex2\n```python\na = np.array([[1], [2], [3]])\nb = np.array([[4], [5], [6]])\nnp.vstack((a,b))\narray([[1],\n       [2],\n       [3],\n       [4],\n       [5],\n       [6]])\n```\nhttps://numpy.org/doc/stable/reference/generated/numpy.vstack.html","metadata":{}},{"cell_type":"markdown","source":"![Alt text](image.png)\n\n","metadata":{}},{"cell_type":"code","source":"\n# Combine the audio signals into a 2D NumPy array (X)\nX = np.vstack((signal_1, signal_2, signal_3))\n\n# Transpose the array so that each row corresponds to a signal\nX = X.T","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <h1 style=\"color:skyblue\" id=\"6\">Applying the ICA</h1>","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import FastICA\n\n# Create an ICA model\nica = FastICA(n_components=3, random_state=0)\n#ica = FastICA(n_components=3, random_state=0, whiten='unit-variance') # to silence the warning\n\n\n# Fit the model to the mixed signals\nsources = ica.fit_transform(X)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sources now contains the result of FastICA, which we hope are the original signals. It's in the shape\nsources.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <h2 style=\"color:skyblue\" id=\"6.1\">Results of the ICA</h2>","metadata":{}},{"cell_type":"code","source":"# Let's split into separate signals and look at them\nsource_1 = sources[:, 0]\nsource_2 = sources[:, 1]\nsource_3 = sources[:, 2]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plotting the results\nax = [None for _ in range(3)]\nplt.figure(figsize=(20, 10))\n\n# Independent Component #1\nax[0] = plt.subplot2grid((3,1), (0,0))\nax[0].plot(source_1, c=\"salmon\")\nax[0].set_title(\"Source 1\")\n\n# Independent Component #2\nax[1] = plt.subplot2grid((3,1), (1,0))\nax[1].plot(source_2, c=\"skyblue\")\nax[1].set_title(\"Source 2\")\n\n# Independent Component #2\nax[2] = plt.subplot2grid((3,1), (2,0))\nax[2].plot(source_3, c=\"lightgreen\")\nax[2].set_title(\"Source 3\")\n\n# setting the ylim \nylim = -0.010, 0.010\nfor i in range(3):\n    ax[i].set_ylim(ylim)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sources","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert the separated audio signals to 16-bit signed integers\nresult_source_1_int = (source_1 * 32767 * 100).astype(np.int16)\nresult_source_2_int = (source_2 * 32767 * 100).astype(np.int16)\nresult_source_3_int = (source_3 * 32767 * 100).astype(np.int16)\n'''\nIt scales the signal values by multiplying them by 32767*100. This scaling is done to map the float values \nto the range of a 16-bit signed integer (-32768 to 32767). Additionally, \nit multiplies by 100 to increase the volume of the signals.  üîä üÜô \n'''","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.io import wavfile\n# Writing wave files\nwavfile.write(\"source1.wav\", fs, result_source_1_int)\nwavfile.write(\"source2.wav\", fs, result_source_2_int)\nwavfile.write(\"source3.wav\", fs, result_source_3_int)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# List of audio file paths\naudio_files = ['source1.wav', 'source2.wav', 'source3.wav']\n\n# Display audio files vertically\nfor audio_file in audio_files:\n    print(audio_file)\n    display(Audio(audio_file))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":">ICA becomes particularly valuable when you have multiple mixed signals (e.g., multiple microphones recording audio in a room with multiple sound sources) and you want to separate those mixed signals into their original independent sources (e.g., separating different speakers' voices from a microphone array).","metadata":{}},{"cell_type":"markdown","source":"# <h1 style=\"color: skyblue\" id=\"7\">PCA</h1>","metadata":{}},{"cell_type":"markdown","source":"# <h1 style=\"color: skyblue\" id=\"8\">MNIST dataset 0Ô∏è‚É£9Ô∏è‚É£</h1>","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nimport matplotlib.image as mpimg\nfrom PIL import Image\nimport seaborn as sns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train \ntrain = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\n# test \ntest = pd.read_csv('/kaggle/input/digit-recognizer/test.csv')\n# train sahpe and test shape\nf'train shape {train.shape}', f'test shape {test.shape}'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.columns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.sample(1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.isnull().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.isnull().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# spliting train to x, y\nX = train.drop(columns=['label'])\ny = train['label']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Functions","metadata":{}},{"cell_type":"code","source":"def show_images(n, dataset=X,  MAX_IMGS=300):\n    num_cols = 10\n    if n % num_cols == 0 and n <= MAX_IMGS:\n        images = dataset.iloc[:n].values.reshape(-1, 28, 28)\n        num_rows = n // num_cols\n        fig, ax = plt.subplots(num_rows, num_cols, figsize=(num_cols, num_rows))\n        for i in range(num_rows):\n            for j in range(num_cols):\n                ax[i, j].imshow(images[i * num_cols + j], cmap='gray')\n                ax[i, j].axis('off')\n        plt.show()\n    else:\n        print('Invalid number of images')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_images(100)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_digits(digit, dataset=X):\n    if digit in range(10):\n        digit_indices = np.where(y == digit)[0]\n        \n        for i in range(50):  # Display the first 50 images of the digit\n            plt.subplot(5, 10, i + 1)\n            imdata = dataset.iloc[digit_indices[i]].values.reshape(28, 28)\n            plt.imshow(imdata, cmap='gray')\n            plt.xticks([])\n            plt.yticks([])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_digits(9)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_digits(6)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_digits(5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import joblib\ndef fit_forest(X, y, save = (False, 'model_digitREC'), plot =True):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=33)\n    clf = RandomForestClassifier(n_estimators=130, max_depth=None)\n    clf.fit(X_train, y_train)\n    # predictions\n    y_pred = clf.predict(X_test)\n    # scoring\n    mat = confusion_matrix(y_test, y_pred)\n    if plot:\n       plt.figure(figsize=(8,8), dpi=170)\n       sns.heatmap(mat, annot=True, linewidths=0.5, cmap='Blues',fmt='d')\n       plt.show()\n    else:\n       print(mat)\n    acc = accuracy_score(y_test, y_pred)\n    print(acc)\n    if save[0]:\n        joblib.dump(clf, f'{save[1]}.joblib')\n    return acc","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fit_forest(X,y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def do_pca(n_component, dataset):\n    X = StandardScaler().fit_transform(dataset)\n    pca = PCA(n_components=n_component)\n    x_pca = pca.fit_transform(X)\n    return pca, x_pca","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca, X_pca = do_pca(2, X)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_pca","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f\"X shape {X.shape}\", f\"X_PCA shape {X_pca.shape}\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fit_forest(X_pca,y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_components(X, y):\n   \n    x_min, x_max = np.min(X, 0), np.max(X, 0)\n    X = (X - x_min) / (x_max - x_min)\n    plt.figure(figsize=(10, 6))\n    for i in range(X.shape[0]):\n        plt.text(X[i, 0], X[i, 1], str(y[i]), color=plt.cm.Set1(y[i]), fontdict={'size': 15})\n\n    plt.xticks([]), plt.yticks([]), plt.ylim([-0.1,1.1]), plt.xlim([-0.1,1.1])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_components(X_pca[:100], y[:100])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accls = [] # accuracy list\ndef tryPCA(start, end, step, clear = False):\n    if clear:\n        accls.clear()\n    for reduced_featured in range(start, end, step):\n        print(f\"number of features {reduced_featured}\")\n        pca, x_pca = do_pca(reduced_featured, X)\n        accls.append(fit_forest(x_pca, y, plot=False))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# this would try to reduce the number of features \n# and see what a number of feature give us meaning \n# so the orginal X features 784 and we got acc 96% \n# with only 18 feature we could reach 91%\n# we can go beyond that 38 feature was 94%\n# we can do even better \n# TODO: I would leave it for you to try and a find if you can do better ü´£üí´\ntryPCA(3, 40, 5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <h1 style=\"color:skyblue\" id=\"9\">KMEANs</h1>","metadata":{}},{"cell_type":"code","source":"import pandas as pd\ncustomers = pd.read_csv('/kaggle/input/customer-segmentation-tutorial-in-python/Mall_Customers.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customers.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customers.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customers.drop(columns='CustomerID').describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customers.describe(include='O')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nsns.pairplot(customers.drop(columns='CustomerID'), hue='Gender');","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customers.drop(columns='CustomerID', inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Encoding\ncustomers['Gender'] = customers['Gender'].astype('category').cat.codes\ncustomers['Gender'].value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customers.columns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numCols = customers.select_dtypes(include='int').columns\nnumCols","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scalling \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import RobustScaler","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dm = customers\ns = MinMaxScaler()\nfor col in numCols:\n    dm[col] = s.fit_transform(dm[col].to_numpy().reshape(-1,1))\ndm","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dr = customers\ns = RobustScaler()\nfor col in numCols:\n    dr[col] = s.fit_transform(dr[col].to_numpy().reshape(-1,1))\ndr","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds = customers\ns = StandardScaler()\nfor col in numCols:\n    ds[col] = s.fit_transform(ds[col].to_numpy().reshape(-1,1))\nds","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(data=ds)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.cluster import KMeans","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_3d_clusters(df, feature_cols, num_clusters):\n    # extract the feature columns from the dataframe\n    X = df[feature_cols]\n\n    # perform K-means clustering with the specified number of clusters\n    kmeans = KMeans(n_clusters=num_clusters, random_state=42).fit(X)\n    df['cluster'] = kmeans.labels_\n\n    # create 3D scatter plot of the clustered data\n    fig = go.Figure(data=[go.Scatter3d(\n        x=df[feature_cols[0]],\n        y=df[feature_cols[1]],\n        z=df[feature_cols[2]],\n        mode='markers',\n        marker=dict(\n            color=df['cluster'],\n            size=10,\n            opacity=0.8\n        )\n    )])\n\n    fig.update_layout(scene=dict(\n        xaxis_title=feature_cols[0],\n        yaxis_title=feature_cols[1],\n        zaxis_title=feature_cols[2]\n    ), title=f'K-means Clustering {num_clusters} Clusters 3D Scatter Plot')\n\n    return fig","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def createKmeanModels(data, mak_k, min_k =1):\n    kmodels = [KMeans(n_clusters=k).fit(data) for k in range(min_k, mak_k)]\n    return kmodels","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(2, 9):\n    fig = plot_3d_clusters(ds, ['Age', 'Annual Income (k$)','Spending Score (1-100)'], num_clusters=i)\n    fig.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def elbow_plot(data, max_k):\n   \n    models = createKmeanModels(data, max_k)\n    distortions = [model.inertia_ for model in models]\n\n    fig = go.Figure()\n    fig.add_trace(go.Scatter(x=list(range(1, max_k+1)), y=distortions, mode='lines+markers'))\n    fig.update_layout(title='Elbow plot for KMeans clustering',\n                      xaxis_title='Number of clusters',\n                      yaxis_title='Distortion')\n\n    return fig","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"elbow_plot(ds, 9)","metadata":{},"execution_count":null,"outputs":[]}]}